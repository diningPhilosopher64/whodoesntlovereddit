{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f7f114-a2fd-4492-bdb1-1428a0c311bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, boto3, pandas as pd, os, sys, pprint\n",
    "from pathlib import Path\n",
    "pp = pprint.PrettyPrinter(indent=2, compact=True, width=80)\n",
    "\n",
    "# from entities.RedditAccount import RedditAccount\n",
    "# from entities.DailyUpload import DailyUpload\n",
    "\n",
    "#TODO: Change path accordingly in handler\n",
    "\n",
    "\n",
    "# Making the current directory in which this file is in discoverable to python.\n",
    "# Commenting it here because it will not work in jupyter notebook. It will work in lambda though.\n",
    "#sys.path.append(os.path.join(os.path.dirname(__file__)))\n",
    "\n",
    "# Below should be used only in jupyter notebook\n",
    "sys.path.append('./')\n",
    "\n",
    "\n",
    "REDDIT_AUTH_URL = 'https://www.reddit.com/api/v1/access_token'\n",
    "REDDIT_ACCOUNTS_TABLE_NAME = 'RedditAccountsTable-dev'\n",
    "DAILY_UPLOADS_TABLE = \"DailyUploadsTable-dev\"\n",
    "REDDIT_API_URL_TOP = \"https://oauth.reddit.com/r/placeholder_value/top\"\n",
    "REDDIT_API_URL_SORT = \"https://oauth.reddit.com/r/placeholder_value/sort\"\n",
    "\n",
    "ddb = boto3.client(\"dynamodb\", region_name=\"ap-south-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b901c5f3-2d8e-43c3-84c1-03d38753ebc7",
   "metadata": {},
   "source": [
    "# Daily Upload Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9194a4-bff5-4596-a510-69dc6056731d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import ddb_helpers\n",
    "\n",
    "\n",
    "class DailyUpload:\n",
    "    post_keys_to_keep = [\n",
    "        \"title\",\n",
    "        \"url\",\n",
    "        \"upvote_ratio\",\n",
    "        \"ups\",\n",
    "        \"author\",\n",
    "        \"name\",\n",
    "        \"total_awards_received\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self, subreddit) -> None:\n",
    "        self.subreddit = subreddit\n",
    "        self.date = str(datetime.today().date())  ## Of the format yyyy-mm-dd\n",
    "        self.total_duration = 0\n",
    "        self.urls = []\n",
    "        # self.df_top = pd.DataFrame()\n",
    "        self.latest_post = None\n",
    "        self.eligible_posts = []\n",
    "\n",
    "    # Renamed from date_subreddit_key()\n",
    "    def key(self) -> dict:\n",
    "        \"\"\"Returns a dictionary with date as PK, subreddit as SK.\n",
    "\n",
    "        Returns:\n",
    "            Dict: Containing serialized subreddit and date\n",
    "        \"\"\"\n",
    "\n",
    "        return {\n",
    "            \"PK\": DailyUpload.__serialize_date(self.date),\n",
    "            \"SK\": DailyUpload.__serialize_subreddit(self.subreddit),\n",
    "        }\n",
    "\n",
    "    # def subreddit_date_key(self) -> dict:\n",
    "    #     \"\"\"Returns a dictionary with subreddit as PK date as SK.\n",
    "\n",
    "    #     Returns:\n",
    "    #         Dict: Containing serialized subreddit and date.\n",
    "    #     \"\"\"\n",
    "    # return {\"PK\": self.__serialize_subreddit(), \"SK\": self.__serialize_date()}\n",
    "\n",
    "    # Renamed from serialize_date_subreddit()\n",
    "    def serialize_to_item(self):\n",
    "        \"\"\"Serializes member variable data of this object for the access pattern:\n",
    "        date-Partition Key\n",
    "        subreddit- Sort Key\n",
    "\n",
    "        Returns:\n",
    "            Dict: Ready to be used by boto3 to insert item into DynamoDB.\n",
    "        \"\"\"\n",
    "        item = self.key()\n",
    "        item[\"posts\"] = DailyUpload.__serialize_posts(self.eligible_posts)\n",
    "        return item\n",
    "\n",
    "    @staticmethod\n",
    "    def __removed_post_is_worthy(post):\n",
    "        if post[\"removed_by\"] or post[\"removal_reason\"]:\n",
    "            if post[\"num_comments\"] > 5 and post[\"score\"] > 10:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    @staticmethod\n",
    "    def __is_eligible(post):\n",
    "        if (\n",
    "            post[\"is_video\"]\n",
    "            and post[\"ups\"] > 0\n",
    "            and post[\"num_comments\"] > 0\n",
    "            and not post[\"over_18\"]\n",
    "            and not post[\"stickied\"]\n",
    "        ):\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def parse_posts(self, posts):\n",
    "        \"\"\"Parse posts and insert into a dataframe.\n",
    "        The last parsed post will updated in a member variable.\n",
    "\n",
    "        Args:\n",
    "            posts (list): List of posts from reddit API\n",
    "        \"\"\"\n",
    "        posts = posts[\"data\"][\"children\"]\n",
    "        for post in posts:\n",
    "            post = post[\"data\"]\n",
    "            self.latest_post = post\n",
    "\n",
    "            if DailyUpload.__is_eligible(post) and DailyUpload.__removed_post_is_worthy(\n",
    "                post\n",
    "            ):\n",
    "\n",
    "                temp = {key: post[key] for key in DailyUpload.post_keys_to_keep}\n",
    "                self.eligible_posts.append(temp)\n",
    "                self.total_duration += int(post[\"media\"][\"reddit_video\"][\"duration\"])\n",
    "                # self.df_top = self.df_top.append(\n",
    "                #     {\n",
    "                #         \"title\": post[\"title\"],\n",
    "                #         \"upvote_ratio\": post[\"upvote_ratio\"],\n",
    "                #         \"ups\": post[\"ups\"],\n",
    "                #         \"downs\": post[\"downs\"],\n",
    "                #         \"score\": post[\"score\"],\n",
    "                #         \"url\": post[\"url\"],\n",
    "                #     },\n",
    "                #     ignore_index=True,\n",
    "                # )\n",
    "\n",
    "    # def sort_and_update_urls(self):\n",
    "    #     self.df_top = self.df_top.sort_values(\n",
    "    #         [\"score\", \"total_awards_received\", \"ups\", \"upvote_ratio\"],\n",
    "    #         ascending=False,\n",
    "    #         axis=0,\n",
    "    #     )\n",
    "\n",
    "    # self.urls = self.urls + self.df_top[\"url\"].tolist()\n",
    "\n",
    "    # def serialize_subreddit_date(self):\n",
    "    #     item = self.subreddit_date_key()\n",
    "    #     item[\"total_duration\"] = self.__serialize_total_duration()\n",
    "    #     return item\n",
    "\n",
    "    # def __serialize_urls(self):\n",
    "    #     serialized_urls = {\"L\": [{\"S\": url} for url in self.urls]}\n",
    "    #     return serialized_urls\n",
    "\n",
    "    #  post[\"title\"],\n",
    "    #                     post[\"upvote_ratio\"],\n",
    "    #                     post[\"ups\"],\n",
    "    #                     post[\"score\"],\n",
    "    #                     post[\"url\"],\n",
    "    #                     post[\"author\"],\n",
    "\n",
    "    @staticmethod\n",
    "    def __serialize_posts(posts):\n",
    "        serialized_posts = {\"L\": [DailyUpload.__serialize_post(post) for post in posts]}\n",
    "\n",
    "        return serialized_posts\n",
    "\n",
    "    @staticmethod\n",
    "    def __serialize_post(post):\n",
    "        serialized_post = {\"M\": {}}\n",
    "\n",
    "        for key in DailyUpload.post_keys_to_keep:\n",
    "            serialized_post[\"M\"][key] = {\n",
    "                ddb_helpers.get_datatype(post[key]): str(post[key])\n",
    "            }\n",
    "\n",
    "        return serialized_post\n",
    "    \n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def __serialize_subreddit(subreddit):\n",
    "        return {\"S\": subreddit}\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def __serialize_date(date):\n",
    "        return {\"S\": date}\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def deserialize_PK_SK_count(item):\n",
    "        deserialized_item = {}\n",
    "        for key, value in item.items():\n",
    "            for _key, _value in value.items():\n",
    "                deserialized_item[key] = _value\n",
    "        return deserialized_item\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91816505-746c-4314-95ac-c8a900942e89",
   "metadata": {},
   "source": [
    "# Reddit Account Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06066cc-4d33-40ff-b38c-aa42bb44a700",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "class RedditAccount:\n",
    "    def __init__(self, subreddit, ddb):\n",
    "        self.subreddit = subreddit\n",
    "        self.client_id = None\n",
    "        self.secret_key = None\n",
    "        self.username = None\n",
    "        self.password = None\n",
    "        self.auth = None\n",
    "        self.headers = {\"User-Agent\": f\"{subreddit}API/0.0.1\"}\n",
    "        self.data = {\"grant_type\": \"password\", \"username\": None, \"password\": None}\n",
    "        self.access_token = None\n",
    "        self.ddb = ddb\n",
    "\n",
    "     \n",
    "    def key(self):\n",
    "        return {\"PK\": {\"S\": self.subreddit}}\n",
    "\n",
    "    def fetch_and_update_account_details(self, REDDIT_ACCOUNTS_TABLE_NAME):\n",
    "        try:\n",
    "            response = self.ddb.get_item(\n",
    "                TableName=REDDIT_ACCOUNTS_TABLE_NAME, Key=self.key()\n",
    "            )\n",
    "            item = RedditAccount.deserialize_item(response[\"Item\"])\n",
    "            self.client_id = item[\"personal_use_script\"]\n",
    "            self.secret_key = item[\"secret_key\"]\n",
    "            self.username = item[\"username\"]\n",
    "            self.password = item[\"password\"]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed with exception: {e}\")\n",
    "\n",
    "        self.data[\"username\"] = self.username\n",
    "        self.data[\"password\"] = self.password\n",
    "\n",
    "    @staticmethod\n",
    "    def deserialize_item(item):\n",
    "        new_item = {}\n",
    "        for key in item:\n",
    "            new_item[key] = RedditAccount.extract_value(item[key])\n",
    "\n",
    "        return new_item\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_value(dictionary):\n",
    "        data_type, value = list(dictionary.keys())[0], list(dictionary.values())[0]\n",
    "\n",
    "        if data_type == \"S\":\n",
    "            return value\n",
    "\n",
    "    def authenticate_with_api(self):\n",
    "        self.auth = requests.auth.HTTPBasicAuth(self.client_id, self.secret_key)\n",
    "\n",
    "    def fetch_and_update_access_token(self, REDDIT_AUTH_URL):\n",
    "        # Authorise and request for access token from Reddit API\n",
    "        res = requests.post(\n",
    "            REDDIT_AUTH_URL, auth=self.auth, data=self.data, headers=self.headers\n",
    "        )\n",
    "        self.access_token = res.json()[\"access_token\"]\n",
    "        self.headers[\"Authorization\"] = f\"bearer {self.access_token}\"\n",
    "\n",
    "    def fetch_posts_as_json(self, url, params={}):\n",
    "        res = requests.get(url, headers=self.headers, params=params)\n",
    "        return res.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a2a386-a53f-4050-91ca-3cdf03deabb0",
   "metadata": {},
   "source": [
    "# Event handler code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4eb313-dbf7-48f1-80c5-bc7448e37c84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3, os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Making the current directory in which this file is in discoverable to python\n",
    "# sys.path.append(os.path.join(os.path.dirname(__file__)))\n",
    "\n",
    "# from entities.DailyUpload import DailyUpload\n",
    "# from entities.RedditAccount import RedditAccount\n",
    "from subreddit_groups import subreddit_groups\n",
    "\n",
    "ddb = boto3.client(\"dynamodb\", region_name=\"ap-south-1\")\n",
    "sqs = boto3.client(\"sqs\")\n",
    "\n",
    "# REDDIT_AUTH_URL = os.getenv(\"REDDIT_AUTH_URL\")\n",
    "# REDDIT_ACCOUNTS_TABLE_NAME = os.getenv(\"REDDIT_ACCOUNTS_TABLE_NAME\")\n",
    "# DAILY_UPLOADS_TABLE = os.getenv(\"DAILY_UPLOADS_TABLE_NAME\")\n",
    "# PROCESS_URLS_FOR_SUBREDDIT_GROUP_QUEUE_URL = os.getenv(\n",
    "#     \"PROCESS_URLS_FOR_SUBREDDIT_GROUP_QUEUE_URL\"\n",
    "# )\n",
    "\n",
    "\n",
    "def run(event, context):\n",
    "\n",
    "    # TODO: Hardcoding subreddit value for now. In production, should extract from queue:\n",
    "    # subreddit = \"funny\"\n",
    "    subreddit = str(event[\"Records\"][0][\"body\"])\n",
    "\n",
    "\n",
    "    # Getting from env here because, if container is warm, it will fetch from the previously\n",
    "    # executed subreddit url.\n",
    "    #TODO: Remove below line and uncomment the one below it.\n",
    "    REDDIT_API_URL_TOP = REDDIT_API_URL_TOP = \"https://oauth.reddit.com/r/placeholder_value/top\"\n",
    "#     REDDIT_API_URL_TOP = os.getenv(\"REDDIT_API_URL_TOP\")\n",
    "    REDDIT_API_URL_TOP = REDDIT_API_URL_TOP.replace(\"placeholder_value\", subreddit)\n",
    "    REDDIT_ACCOUNTS_TABLE_NAME = 'RedditAccountsTable-dev'\n",
    "\n",
    "    daily_upload = DailyUpload(subreddit=subreddit)\n",
    "    reddit_account = RedditAccount(\n",
    "        subreddit=subreddit,\n",
    "        ddb=ddb)\n",
    "    \n",
    "    reddit_account.fetch_and_update_account_details(REDDIT_ACCOUNTS_TABLE_NAME)\n",
    "    reddit_account.authenticate_with_api()\n",
    "    reddit_account.fetch_and_update_access_token(REDDIT_AUTH_URL)\n",
    "    \n",
    "#         REDDIT_ACCOUNTS_TABLE_NAME=REDDIT_ACCOUNTS_TABLE_NAME,\n",
    "#         REDDIT_AUTH_URL=REDDIT_AUTH_URL,\n",
    "    \n",
    "    print(f\"Subreddit : {subreddit} is being processed\")\n",
    "\n",
    "    # Keep fetching and parsing posts from reddit api till daily_upload.total_duration\n",
    "    # is more than 600 seconds. Will use the 'after' param to keep going backwards.\n",
    "    after = None\n",
    "    while daily_upload.total_duration < 601:\n",
    "        print(f\"Fetching {subreddit} posts after {after}\")\n",
    "        posts = reddit_account.fetch_posts_as_json(\n",
    "            REDDIT_API_URL_TOP, params={\"limit\": \"100\", \"after\": after}\n",
    "        )\n",
    "        daily_upload.parse_posts(posts)\n",
    "        after = daily_upload.latest_post[\"name\"]\n",
    "        print(\" total duration \",daily_upload.total_duration)\n",
    "        print(\" eligible posts len:  \", len(daily_upload.eligible_posts))\n",
    "\n",
    "    # After uploading this subreddits' urls, update the count of todays_subreddits_count\n",
    "    # doing this as a transaction.\n",
    "    try:\n",
    "        res = ddb.transact_write_items(\n",
    "            TransactItems=[\n",
    "                {\n",
    "                    \"Put\": {\n",
    "                        \"TableName\": DAILY_UPLOADS_TABLE,\n",
    "                        \"Item\": daily_upload.serialize_to_item(),\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"Update\": {\n",
    "                        \"TableName\": DAILY_UPLOADS_TABLE,\n",
    "                        \"Key\": {\n",
    "                            \"PK\": {\"S\": daily_upload.date},\n",
    "                            \"SK\": {\"S\": \"todays_subreddits_count\"},\n",
    "                        },\n",
    "                        \"ConditionExpression\": \"attribute_exists(PK) and attribute_exists(SK)\",\n",
    "                        \"UpdateExpression\": \"SET #count = #count + :inc\",\n",
    "                        \"ExpressionAttributeNames\": {\"#count\": \"count\"},\n",
    "                        \"ExpressionAttributeValues\": {\":inc\": {\"N\": \"1\"}},\n",
    "                    }\n",
    "                },\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if res[\"ResponseMetadata\"][\"HTTPStatusCode\"] != 200:\n",
    "            raise Exception(\n",
    "                f\"Failed to write transaction for {subreddit} on {daily_upload.date}\"\n",
    "            )\n",
    "\n",
    "        print(f\"Successfully updated DB with posts for {subreddit} subreddit\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return {\"error\": e}\n",
    "\n",
    "    # Prepping up for fetching todays_subreddits_count an total_subreddits_count from DailyUploads table.\n",
    "    key = daily_upload.key()\n",
    "\n",
    "    total_subreddits_key = daily_upload.key()\n",
    "    todays_subreddits_key = daily_upload.key()\n",
    "\n",
    "    total_subreddits_key[\"SK\"][\"S\"] = \"total_subreddits_count\"\n",
    "    todays_subreddits_key[\"SK\"][\"S\"] = \"todays_subreddits_count\"\n",
    "\n",
    "    try:\n",
    "        res = ddb.transact_get_items(\n",
    "            TransactItems=[\n",
    "                {\n",
    "                    \"Get\": {\n",
    "                        \"Key\": total_subreddits_key,\n",
    "                        \"TableName\": DAILY_UPLOADS_TABLE,\n",
    "                    },\n",
    "                },\n",
    "                {\n",
    "                    \"Get\": {\n",
    "                        \"Key\": todays_subreddits_key,\n",
    "                        \"TableName\": DAILY_UPLOADS_TABLE,\n",
    "                    },\n",
    "                },\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return {\"error\": e}\n",
    "\n",
    "    print(f\"{subreddit} subreddit has updated todays_subreddit_count \")\n",
    "    # Extract items from response\n",
    "    print( \"Response is : \", res)\n",
    "    items = [response[\"Item\"] for response in res[\"Responses\"]]\n",
    "\n",
    "    # Deserialize the items and extract total_subreddits_count and todays_subreddits_count items.\n",
    "    total_subreddits_item_deserialized, todays_subreddit_item_deserialized = [\n",
    "        DailyUpload.deserialize_PK_SK_count(item) for item in items\n",
    "    ]\n",
    "\n",
    "    # If evaluates to true, then push subreddit groups to ProcessUrlsQueue\n",
    "    if (\n",
    "        total_subreddits_item_deserialized[\"count\"]\n",
    "        == todays_subreddit_item_deserialized[\"count\"]\n",
    "    ):\n",
    "        push_subreddit_groups_to_queue()\n",
    "\n",
    "        # and then send custom response to show today's urls\n",
    "        # of all subreddits have been processed.\n",
    "        return {\n",
    "            \"success\": f\"All subreddits have been processed and uploaded urls for date {daily_upload.date}.\\nPushed subreddit_groups to: {PROCESS_URLS_QUEUE_URL}\"\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        subreddit: f\"successfully processed {subreddit} for date: {daily_upload.date}\"\n",
    "    }\n",
    "\n",
    "\n",
    "def push_subreddit_groups_to_queue():\n",
    "    for group in subreddit_groups:\n",
    "        res = sqs.send_message(\n",
    "            QueueUrl=PROCESS_URLS_FOR_SUBREDDIT_GROUP_QUEUE_URL, MessageBody=group\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5625d0d-7c73-4aeb-8bf1-ce48b8f0d477",
   "metadata": {},
   "outputs": [],
   "source": [
    "event = {\"Records\": [ {\"body\": \"funny\"} ]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6528bca5-72e4-4def-a146-a9efe54ee16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(event, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7f8dfd-e4f7-42aa-98be-77bb8832f375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96ce25a-a3a6-49a0-b842-f129309afa07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "203dc3cd-cc07-484e-93ea-6daa6dade353",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pushshift api tryout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31118117-2a1f-49a6-ba21-9163a3c27bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = PushshiftAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6780af-ffbd-4d76-868e-88cd716a727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1335ff-b6f0-45fb-8bc6-efdecbe2c0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "today =  datetime.today()\n",
    "yesterday = datetime.today() - timedelta(days=1)\n",
    "day_before_yesterday = yesterday - timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1d29ae-d9b8-4cf4-a259-3ef9a9a0f556",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime(today.year, today.month, today.day, 0,0,0).timestamp()\n",
    "yesterday = datetime(yesterday.year, yesterday.month, yesterday.day,0,0,0).timestamp()\n",
    "day_before_yesterday = datetime(day_before_yesterday.year, day_before_yesterday.month, day_before_yesterday.day,0,0,0).timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4987782-469c-449b-a215-71e1391add11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " a = list(api.search_submissions(after=day_before_yesterday, before=today, subreddit='funny', filter=['url', 'title'], limit = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5525ffc-0bf2-4f0b-aba8-f9eb5db3fecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "int(day_before_yesterday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b94ffc6-7d9b-488d-a1eb-bd06047d62d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_pushshift_data(data_type, **kwargs):\n",
    "    \"\"\"\n",
    "    Gets data from the pushshift api.\n",
    " \n",
    "    data_type can be 'comment' or 'submission'\n",
    "    The rest of the args are interpreted as payload.\n",
    " \n",
    "    Read more: https://github.com/pushshift/api\n",
    "    \"\"\"\n",
    " \n",
    "    base_url = f\"https://api.pushshift.io/reddit/search/submission/?subreddit=funny&num_comments=>0&after={int(day_before_yesterday)}&before={int(yesterday)}&is_video=true&sort_type=score&sort=score:asc&size=100&aggs=subreddit\"\n",
    "#     payload = {}\n",
    "#     print(payload)\n",
    "    request = requests.get(base_url)\n",
    "    return request.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7162332a-ce32-487a-91f1-729889c4332d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type=\"submission\"     # give me comments, use \"submission\" to publish something\n",
    "query=\"funny\"          # Add your query\n",
    "duration=\"1d\"          # Select the timeframe. Epoch value or Integer + \"s,m,h,d\" (i.e. \"second\", \"minute\", \"hour\", \"day\")\n",
    "size=1000               # maximum 1000 comments\n",
    "sort_type=\"score\"       # Sort by score (Accepted: \"score\", \"num_comments\", \"created_utc\")\n",
    "sort=\"desc\"             # sort descending\n",
    "aggs=\"subreddit\"        #\"author\", \"link_id\", \"created_utc\", \"subreddit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3645ffb5-554d-47e6-a73d-f52197fb5df7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_url = f\"https://api.pushshift.io/reddit/search/submission/?subreddit=funny&num_comments=>0&over_18=false&after={int(day_before_yesterday)}&before={int(yesterday)}&is_video=true&sort_type=score&sort=score:desc&size=100&aggs=subreddit\"\n",
    "\n",
    "request = requests.get(base_url)\n",
    "b = request.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b29ba50-c3fd-4169-8f27-6ee104843dbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b['data'][59]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f08483-3dd1-4000-b8bd-f14bd76c31de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for post in a['data']:    \n",
    "    pp.pprint(post)\n",
    "    break\n",
    "\n",
    "   \n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922df849-2007-41e7-869d-9a3cd2d06835",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c136a5-271f-49c5-8336-8f1f4f540a27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3810jvsc74a57bd024b555649d86f360bc29675e4ea05747664583e96f16283f97272f664e30bb4e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
