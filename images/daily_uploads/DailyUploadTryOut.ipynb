{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16f7f114-a2fd-4492-bdb1-1428a0c311bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, boto3, pandas as pd, os, sys, pprint\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# from entities.RedditAccount import RedditAccount\n",
    "# from entities.DailyUpload import DailyUpload\n",
    "\n",
    "#TODO: Change path accordingly in handler\n",
    "\n",
    "\n",
    "# Making the current directory in which this file is in discoverable to python.\n",
    "# Commenting it here because it will not work in jupyter notebook. It will work in lambda though.\n",
    "#sys.path.append(os.path.join(os.path.dirname(__file__)))\n",
    "\n",
    "# Below should be used only in jupyter notebook\n",
    "sys.path.append('../')\n",
    "\n",
    "\n",
    "REDDIT_AUTH_URL = 'https://www.reddit.com/api/v1/access_token'\n",
    "REDDIT_ACCOUNTS_TABLE_NAME = 'RedditAccountsTable-dev'\n",
    "DAILY_UPLOADS_TABLE = \"DailyUploadsTable-dev\"\n",
    "REDDIT_API_URL_TOP = \"https://oauth.reddit.com/r/placeholder_value/top\"\n",
    "REDDIT_API_URL_SORT = \"https://oauth.reddit.com/r/placeholder_value/sort\"\n",
    "\n",
    "ddb = boto3.client(\"dynamodb\", region_name=\"ap-south-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b901c5f3-2d8e-43c3-84c1-03d38753ebc7",
   "metadata": {},
   "source": [
    "# GatherUrls Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf9194a4-bff5-4596-a510-69dc6056731d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2, compact=True, width=80)\n",
    "\n",
    "\n",
    "class GatherUrls:\n",
    "    post_keys_to_keep = [\n",
    "        \"title\",\n",
    "        \"url\",\n",
    "        \"upvote_ratio\",\n",
    "        \"ups\",\n",
    "        \"author\",\n",
    "        \"name\",\n",
    "        \"total_awards_received\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self, subreddit, logger) -> None:\n",
    "        self.subreddit = subreddit\n",
    "        self.date = str(datetime.today().date())  ## Of the format yyyy-mm-dd\n",
    "        self.total_duration = 0\n",
    "        self.urls = []\n",
    "        self.latest_post = None\n",
    "        self.eligible_posts = []\n",
    "        self.logger = logger\n",
    "\n",
    "    # Renamed from date_subreddit_key()\n",
    "    def key(self) -> dict:\n",
    "        \"\"\"Returns a dictionary with date as PK, subreddit as SK.\n",
    "\n",
    "        Returns:\n",
    "            Dict: Containing serialized subreddit and date\n",
    "        \"\"\"\n",
    "\n",
    "        return {\n",
    "            \"PK\": GatherUrls.__serialize_date(self.date),\n",
    "            \"SK\": GatherUrls.__serialize_subreddit(self.subreddit),\n",
    "        }\n",
    "\n",
    "    def serialize_to_item(self):\n",
    "        \"\"\"Serializes member variable data of this object for the access pattern:\n",
    "        date-Partition Key\n",
    "        subreddit- Sort Key\n",
    "\n",
    "        Returns:\n",
    "            Dict: Ready to be used by boto3 to insert item into DynamoDB.\n",
    "        \"\"\"\n",
    "        item = self.key()\n",
    "        item[\"posts\"] = GatherUrls.__serialize_posts(self.eligible_posts)\n",
    "        self.logger.info(\"Serialized item is:\\n\")\n",
    "        self.logger.info(pp.pformat(item))\n",
    "        return item\n",
    "\n",
    "    @staticmethod\n",
    "    def __removed_post_is_worthy(post):\n",
    "        if post[\"removed_by\"] or post[\"removal_reason\"]:\n",
    "            if post[\"num_comments\"] > 5 and post[\"score\"] > 10:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    @staticmethod\n",
    "    def __is_eligible(post):\n",
    "        if post[\"is_video\"] and not post[\"over_18\"] and not post[\"stickied\"]:\n",
    "            if post[\"total_awards_received\"] > 0:\n",
    "                return True\n",
    "\n",
    "            if post[\"ups\"] > 0 and post[\"num_comments\"] > 0:\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def parse_posts(self, posts):\n",
    "        \"\"\"Parse posts and insert into a dataframe.\n",
    "        The last parsed post will updated in a member variable.\n",
    "\n",
    "        Args:\n",
    "            posts (list): List of posts from reddit API\n",
    "        \"\"\"\n",
    "        posts = posts[\"data\"][\"children\"]\n",
    "        self.logger.info(f\"For {self.subreddit} on date: {self.date}\")\n",
    "        duration = 0\n",
    "        for post in posts:\n",
    "            post = post[\"data\"]\n",
    "            self.latest_post = post\n",
    "            if GatherUrls.__is_eligible(post) and GatherUrls.__removed_post_is_worthy(\n",
    "                post\n",
    "            ):\n",
    "\n",
    "                temp = {key: post[key] for key in GatherUrls.post_keys_to_keep}\n",
    "                self.eligible_posts.append(temp)\n",
    "                duration = int(post[\"media\"][\"reddit_video\"][\"duration\"])\n",
    "                self.total_duration += duration\n",
    "                self.logger.info(\n",
    "                    f\"Post:\\nTitle: {post['title']}\\nDuration: {duration}s\\nwas added to eligible posts\\n\"\n",
    "                )\n",
    "\n",
    "        self.logger.info(\n",
    "            f\"Total duration for {self.subreddit} subreddit on {self.date} is {self.total_duration}\\n\"\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def __serialize_posts(posts):\n",
    "        serialized_posts = {\"L\": [GatherUrls.__serialize_post(post) for post in posts]}\n",
    "        return serialized_posts\n",
    "\n",
    "    @staticmethod\n",
    "    def deserialize_from_item(serialized_item):\n",
    "        deserialized_item = {}\n",
    "        serialized_item = serialized_item[\"Item\"]\n",
    "\n",
    "        for key, value in serialized_item.items():\n",
    "            for _key, _value in value.items():\n",
    "                deserialized_item[key] = helpers.ddb.deserialize_piece_of_item(\n",
    "                    _key, _value\n",
    "                )\n",
    "\n",
    "        return deserialized_item\n",
    "\n",
    "    @staticmethod\n",
    "    def __serialize_post(post):\n",
    "        serialized_post = {\"M\": {}}\n",
    "\n",
    "        for key in GatherUrls.post_keys_to_keep:\n",
    "            serialized_post[\"M\"][key] = {\n",
    "                helpers.ddb.get_datatype(post[key]): str(post[key])\n",
    "            }\n",
    "\n",
    "        return serialized_post\n",
    "\n",
    "    @staticmethod\n",
    "    def __serialize_subreddit(subreddit):\n",
    "        return {\"S\": subreddit}\n",
    "\n",
    "    @staticmethod\n",
    "    def __serialize_date(date):\n",
    "        return {\"S\": date}\n",
    "\n",
    "    @staticmethod\n",
    "    def deserialize_PK_SK_count(item):\n",
    "        deserialized_item = {}\n",
    "        for key, value in item.items():\n",
    "            for _key, _value in value.items():\n",
    "                deserialized_item[key] = _value\n",
    "        return deserialized_item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91816505-746c-4314-95ac-c8a900942e89",
   "metadata": {},
   "source": [
    "# Reddit Account Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b06066cc-4d33-40ff-b38c-aa42bb44a700",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests, logging, pprint\n",
    "from helpers.Exceptions import InvalidCredentialsProvidedException\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2, compact=True, width=80)\n",
    "\n",
    "\n",
    "class RedditAccount:\n",
    "    def __init__(self, subreddit, ddb, logger):\n",
    "        self.subreddit = subreddit\n",
    "        self.client_id = None\n",
    "        self.secret_key = None\n",
    "        self.username = None\n",
    "        self.password = None\n",
    "        self.auth = None\n",
    "        self.headers = {\"User-Agent\": f\"{subreddit}API/0.0.1\"}\n",
    "        self.data = {\"grant_type\": \"password\", \"username\": None, \"password\": None}\n",
    "        self.access_token = None\n",
    "        self.ddb = ddb\n",
    "        self.logger = logger\n",
    "\n",
    "    def key(self):\n",
    "        return {\"PK\": {\"S\": self.subreddit}}\n",
    "\n",
    "    def fetch_and_update_account_details(self, REDDIT_ACCOUNTS_TABLE_NAME):\n",
    "        params = {\"TableName\": REDDIT_ACCOUNTS_TABLE_NAME, \"Key\": self.key()}\n",
    "        item = ddb_helpers.get_item(ddb=self.ddb, logger=self.logger, **params)   \n",
    "        deserialized_item = RedditAccount.deserialize_item(item)       \n",
    "        \n",
    "        self.client_id = deserialized_item[\"personal_use_script\"]\n",
    "        self.secret_key = deserialized_item[\"secret_key\"]\n",
    "        self.username = deserialized_item[\"username\"]\n",
    "        self.password = deserialized_item[\"password\"]\n",
    "        self.data[\"username\"] = self.username\n",
    "        self.data[\"password\"] = self.password\n",
    "        self.logger.info(\"Fetched and updated the following account details:\\n\")\n",
    "        self.logger.info(pp.pformat(deserialized_item))\n",
    "\n",
    "    @staticmethod\n",
    "    def deserialize_item(item):\n",
    "        deserialized_item = {}\n",
    "        for key, value in item.items():\n",
    "            for _key, _value in value.items():\n",
    "                deserialized_item[key] = ddb_helpers.deserialize_piece_of_item(_key, _value)\n",
    "        \n",
    "\n",
    "        return deserialized_item\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_value(dictionary):\n",
    "        data_type, value = list(dictionary.keys())[0], list(dictionary.values())[0]\n",
    "\n",
    "        if data_type == \"S\":\n",
    "            return value\n",
    "\n",
    "    def authenticate_with_api(self):\n",
    "        self.auth = requests.auth.HTTPBasicAuth(self.client_id, self.secret_key)\n",
    "\n",
    "    def fetch_and_update_access_token(self, REDDIT_AUTH_URL):\n",
    "        try:\n",
    "            # Authorise and request for access token from Reddit API\n",
    "            res = requests.post(\n",
    "                REDDIT_AUTH_URL, auth=self.auth, data=self.data, headers=self.headers\n",
    "            )\n",
    "\n",
    "            res = res.json()\n",
    "            if \"error\" in res and res[\"error\"] == 401:\n",
    "                raise InvalidCredentialsProvidedException()\n",
    "\n",
    "        except (InvalidCredentialsProvidedException, Exception):\n",
    "            self.logger.error(f\"Response object contains:\\n\")\n",
    "            self.logger.error(pp.pformat(res))\n",
    "            self.logger.error(\n",
    "                \"Invalid Credentials. The following details were provided:\\n\"\n",
    "            )\n",
    "            self.logger.error(\n",
    "                f\"Requests auth object:\\nusername: {self.auth.username}\\npassword: {self.auth.password}\\n\"\n",
    "            )\n",
    "            self.logger.error(f\"Data provided in the POST request:\\n\")\n",
    "            self.logger.error(pp.pformat(self.headers))\n",
    "            self.logger.error(f\"Headers present in the POST request:\\n\")\n",
    "            self.logger.error(pp.pformat(self.headers))\n",
    "\n",
    "        self.access_token = res[\"access_token\"]\n",
    "        self.headers[\"Authorization\"] = f\"bearer {self.access_token}\"\n",
    "\n",
    "    def fetch_posts_as_json(self, url, params={}):\n",
    "        try:\n",
    "            res = requests.get(url, headers=self.headers, params=params)\n",
    "            return res.json()\n",
    "\n",
    "        except Exception as err:\n",
    "            self.logger.error(f\"Unable to fetch posts from Reddit\")\n",
    "            self.logger.error(\"Headers used:\\n\")\n",
    "            self.logger.error(pp.pformat(self.headers))\n",
    "            self.logger.error(f\"URL to fetch posts from: {url}\\n\")\n",
    "            self.logger.error(\"params passed were:\\n\")\n",
    "            self.logger.error(pp.pformat(params))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a2a386-a53f-4050-91ca-3cdf03deabb0",
   "metadata": {},
   "source": [
    "# Event handler code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a4eb313-dbf7-48f1-80c5-bc7448e37c84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3, os, sys, logging, pprint\n",
    "from pathlib import Path\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2, compact=True, width=80)\n",
    "\n",
    "# Initialize log config.\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Making the current directory in which this file is in discoverable to python\n",
    "# sys.path.append(os.path.join(os.path.dirname(__file__)))\n",
    "\n",
    "# TODO: Commenting this only for jupyter\n",
    "# from entities.GatherUrls import GatherUrls\n",
    "# from entities.RedditAccount import RedditAccount\n",
    "from helpers import ddb as ddb_helpers\n",
    "\n",
    "# from subreddit_groups import subreddit_groups\n",
    "\n",
    "ddb = boto3.client(\"dynamodb\", region_name=\"ap-south-1\")\n",
    "sqs = boto3.client(\"sqs\")\n",
    "\n",
    "# REDDIT_AUTH_URL = os.getenv(\"REDDIT_AUTH_URL\")\n",
    "# REDDIT_ACCOUNTS_TABLE_NAME = os.getenv(\"REDDIT_ACCOUNTS_TABLE_NAME\")\n",
    "# DAILY_UPLOADS_TABLE_NAME = os.getenv(\"DAILY_UPLOADS_TABLE_NAME\")\n",
    "# PROCESS_URLS_FOR_SUBREDDIT_GROUP_QUEUE_URL = os.getenv(\n",
    "#     \"PROCESS_URLS_FOR_SUBREDDIT_GROUP_QUEUE_URL\"\n",
    "# )\n",
    "\n",
    "\n",
    "def run(event, context):\n",
    "\n",
    "    # TODO: Hardcoding subreddit value for now. In production, should extract from queue:\n",
    "    # subreddit = \"funny\"\n",
    "    subreddit = str(event[\"Records\"][0][\"body\"])\n",
    "    logger.info(f\"Subreddit : {subreddit}, is being processed\")\n",
    "\n",
    "    # Getting from env here because, if container is warm, it will fetch from the previously\n",
    "    # executed subreddit url.\n",
    "#     REDDIT_API_URL_TOP = os.getenv(\"REDDIT_API_URL_TOP\")\n",
    "    #TODO: Uncomment above and comment below one. This change is only for jupyter\n",
    "    REDDIT_API_URL_TOP = \"https://oauth.reddit.com/r/placeholder_value/top\"\n",
    "    REDDIT_API_URL_TOP = REDDIT_API_URL_TOP.replace(\"placeholder_value\", subreddit)\n",
    "\n",
    "    gather_urls = GatherUrls(subreddit=subreddit, logger=logger)\n",
    "    reddit_account = RedditAccount(subreddit=subreddit, ddb=ddb, logger=logger)\n",
    "\n",
    "    reddit_account.fetch_and_update_account_details(REDDIT_ACCOUNTS_TABLE_NAME)\n",
    "    reddit_account.authenticate_with_api()\n",
    "    reddit_account.fetch_and_update_access_token(REDDIT_AUTH_URL)\n",
    "\n",
    "    # Keep fetching and parsing posts from reddit api till gather_urls.total_duration\n",
    "    # is more than 600 seconds. Will use the 'after' param to keep going backwards.\n",
    "    after = None\n",
    "    while gather_urls.total_duration < 601:\n",
    "        logger.info(f\"Fetching {subreddit} posts after {after}\")\n",
    "        posts = reddit_account.fetch_posts_as_json(\n",
    "            REDDIT_API_URL_TOP, params={\"limit\": \"100\", \"after\": after}\n",
    "        )\n",
    "        gather_urls.parse_posts(posts)\n",
    "        after = gather_urls.latest_post[\"name\"]\n",
    "\n",
    "    # After uploading this subreddits' urls, update the count of todays_subreddits_count\n",
    "    # doing this as a transaction.\n",
    "    params = {\n",
    "        \"TransactItems\": [\n",
    "            {\n",
    "                \"Put\": {\n",
    "                    \"TableName\": DAILY_UPLOADS_TABLE_NAME,\n",
    "                    \"Item\": gather_urls.serialize_to_item(),\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"Update\": {\n",
    "                    \"TableName\": DAILY_UPLOADS_TABLE_NAME,\n",
    "                    \"Key\": {\n",
    "                        \"PK\": {\"S\": gather_urls.date},\n",
    "                        \"SK\": {\"S\": \"todays_subreddits_count\"},\n",
    "                    },\n",
    "                    \"ConditionExpression\": \"attribute_exists(PK) and attribute_exists(SK)\",\n",
    "                    \"UpdateExpression\": \"SET #count = #count + :inc\",\n",
    "                    \"ExpressionAttributeNames\": {\"#count\": \"count\"},\n",
    "                    \"ExpressionAttributeValues\": {\":inc\": {\"N\": \"1\"}},\n",
    "                }\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    res = ddb_helpers.transact_write_items(ddb, logger, **params)\n",
    "\n",
    "    logger.info(\n",
    "        f\"Successfully updated DB for {subreddit} subreddit on {gather_urls.date}\"\n",
    "    )\n",
    "   \n",
    "\n",
    "    return {\n",
    "        subreddit: f\"successfully processed {subreddit} for date: {gather_urls.date}\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5625d0d-7c73-4aeb-8bf1-ce48b8f0d477",
   "metadata": {},
   "outputs": [],
   "source": [
    "event = {\"Records\": [ {\"body\": \"funny\"} ]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6528bca5-72e4-4def-a146-a9efe54ee16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Subreddit : funny, is being processed\n",
      "INFO:root:Received the following item:\n",
      "INFO:root:{ 'PK': {'S': 'funny'},\n",
      "  'email_address': {'S': 'mugblsxlqpoqbclaqi@bptfp.net'},\n",
      "  'password': {'S': 'Abcd@12349'},\n",
      "  'personal_use_script': {'S': 'epsNZpypuQ1IngadWDnlGg'},\n",
      "  'secret_key': {'S': 'yG4Ej57nkBMIdFzpWkzhNXChnsiluw'},\n",
      "  'username': {'S': 'mugblsxlqpoqbclaqi'}}\n",
      "INFO:root:Fetched and updated the following account details:\n",
      "\n",
      "INFO:root:{ 'PK': 'funny',\n",
      "  'email_address': 'mugblsxlqpoqbclaqi@bptfp.net',\n",
      "  'password': 'Abcd@12349',\n",
      "  'personal_use_script': 'epsNZpypuQ1IngadWDnlGg',\n",
      "  'secret_key': 'yG4Ej57nkBMIdFzpWkzhNXChnsiluw',\n",
      "  'username': 'mugblsxlqpoqbclaqi'}\n",
      "INFO:root:Fetching funny posts after None\n",
      "INFO:root:For funny on date: 2021-08-15\n",
      "INFO:root:Post:\n",
      "Title: No fucks given at the Home Depot today. 🤣 I'm envious of this man's confidence.\n",
      "Duration: 4s\n",
      "was added to eligible posts\n",
      "\n",
      "INFO:root:Post:\n",
      "Title: Apple: We're not scanning your images, we're just scanning your images.\n",
      "Duration: 38s\n",
      "was added to eligible posts\n",
      "\n",
      "INFO:root:Post:\n",
      "Title: Protect yourself !\n",
      "Duration: 62s\n",
      "was added to eligible posts\n",
      "\n",
      "INFO:root:Post:\n",
      "Title: We all know what the punchline is going to be.\n",
      "Duration: 78s\n",
      "was added to eligible posts\n",
      "\n",
      "INFO:root:Post:\n",
      "Title: The captain must go down with the ship!\n",
      "Duration: 189s\n",
      "was added to eligible posts\n",
      "\n",
      "INFO:root:Post:\n",
      "Title: The fastest way to connect to your past lives.\n",
      "Duration: 8s\n",
      "was added to eligible posts\n",
      "\n",
      "INFO:root:Post:\n",
      "Title: Can relate (sound on)\n",
      "Duration: 24s\n",
      "was added to eligible posts\n",
      "\n",
      "INFO:root:Post:\n",
      "Title: Disappointed🤣\n",
      "Duration: 9s\n",
      "was added to eligible posts\n",
      "\n",
      "INFO:root:Post:\n",
      "Title: Armed robber uses hand sanitizer before robbing a pharmacy\n",
      "Duration: 35s\n",
      "was added to eligible posts\n",
      "\n",
      "INFO:root:Post:\n",
      "Title: An attempt at indoor climbing\n",
      "Duration: 7s\n",
      "was added to eligible posts\n",
      "\n",
      "INFO:root:Post:\n",
      "Title: solving a crossword with Dom\n",
      "Duration: 103s\n",
      "was added to eligible posts\n",
      "\n",
      "INFO:root:Post:\n",
      "Title: You can't park there, sir!\n",
      "Duration: 85s\n",
      "was added to eligible posts\n",
      "\n",
      "INFO:root:Post:\n",
      "Title: Power generator on life support\n",
      "Duration: 15s\n",
      "was added to eligible posts\n",
      "\n",
      "INFO:root:Post:\n",
      "Title: This is how my cat sits on the sofa 😂🤣🐱😹\n",
      "Duration: 14s\n",
      "was added to eligible posts\n",
      "\n",
      "INFO:root:Post:\n",
      "Title: PUT IT DOWN\n",
      "Duration: 22s\n",
      "was added to eligible posts\n",
      "\n",
      "INFO:root:Post:\n",
      "Title: Local Dunkin Donuts, It was having a party of its own.\n",
      "Duration: 7s\n",
      "was added to eligible posts\n",
      "\n",
      "INFO:root:Post:\n",
      "Title: These guys are called KRPP or Kiddy Ride Police Patrol and ride around town. Checking up on kiddy rides, scootmobiles and cute pets. They have their own soundtrack.\n",
      "Duration: 23s\n",
      "was added to eligible posts\n",
      "\n",
      "INFO:root:Post:\n",
      "Title: You and i have unfinished buisness\n",
      "Duration: 12s\n",
      "was added to eligible posts\n",
      "\n",
      "INFO:root:Post:\n",
      "Title: Kick Buttowski 🤟\n",
      "Duration: 14s\n",
      "was added to eligible posts\n",
      "\n",
      "INFO:root:Post:\n",
      "Title: Was not expecting that.\n",
      "Duration: 9s\n",
      "was added to eligible posts\n",
      "\n",
      "INFO:root:Total duration for funny subreddit on 2021-08-15 is 758\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'DAILY_UPLOADS_TABLE_NAME' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_53752/2354267148.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_53752/2511468851.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(event, context)\u001b[0m\n\u001b[1;32m     67\u001b[0m             {\n\u001b[1;32m     68\u001b[0m                 \"Put\": {\n\u001b[0;32m---> 69\u001b[0;31m                     \u001b[0;34m\"TableName\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDAILY_UPLOADS_TABLE_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m                     \u001b[0;34m\"Item\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgather_urls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserialize_to_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DAILY_UPLOADS_TABLE_NAME' is not defined"
     ]
    }
   ],
   "source": [
    "run(event, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7f8dfd-e4f7-42aa-98be-77bb8832f375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96ce25a-a3a6-49a0-b842-f129309afa07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "203dc3cd-cc07-484e-93ea-6daa6dade353",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pushshift api tryout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31118117-2a1f-49a6-ba21-9163a3c27bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = PushshiftAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6780af-ffbd-4d76-868e-88cd716a727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1335ff-b6f0-45fb-8bc6-efdecbe2c0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "today =  datetime.today()\n",
    "yesterday = datetime.today() - timedelta(days=1)\n",
    "day_before_yesterday = yesterday - timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1d29ae-d9b8-4cf4-a259-3ef9a9a0f556",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime(today.year, today.month, today.day, 0,0,0).timestamp()\n",
    "yesterday = datetime(yesterday.year, yesterday.month, yesterday.day,0,0,0).timestamp()\n",
    "day_before_yesterday = datetime(day_before_yesterday.year, day_before_yesterday.month, day_before_yesterday.day,0,0,0).timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4987782-469c-449b-a215-71e1391add11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " a = list(api.search_submissions(after=day_before_yesterday, before=today, subreddit='funny', filter=['url', 'title'], limit = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5525ffc-0bf2-4f0b-aba8-f9eb5db3fecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "int(day_before_yesterday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b94ffc6-7d9b-488d-a1eb-bd06047d62d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_pushshift_data(data_type, **kwargs):\n",
    "    \"\"\"\n",
    "    Gets data from the pushshift api.\n",
    " \n",
    "    data_type can be 'comment' or 'submission'\n",
    "    The rest of the args are interpreted as payload.\n",
    " \n",
    "    Read more: https://github.com/pushshift/api\n",
    "    \"\"\"\n",
    " \n",
    "    base_url = f\"https://api.pushshift.io/reddit/search/submission/?subreddit=funny&num_comments=>0&after={int(day_before_yesterday)}&before={int(yesterday)}&is_video=true&sort_type=score&sort=score:asc&size=100&aggs=subreddit\"\n",
    "#     payload = {}\n",
    "#     print(payload)\n",
    "    request = requests.get(base_url)\n",
    "    return request.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7162332a-ce32-487a-91f1-729889c4332d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type=\"submission\"     # give me comments, use \"submission\" to publish something\n",
    "query=\"funny\"          # Add your query\n",
    "duration=\"1d\"          # Select the timeframe. Epoch value or Integer + \"s,m,h,d\" (i.e. \"second\", \"minute\", \"hour\", \"day\")\n",
    "size=1000               # maximum 1000 comments\n",
    "sort_type=\"score\"       # Sort by score (Accepted: \"score\", \"num_comments\", \"created_utc\")\n",
    "sort=\"desc\"             # sort descending\n",
    "aggs=\"subreddit\"        #\"author\", \"link_id\", \"created_utc\", \"subreddit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3645ffb5-554d-47e6-a73d-f52197fb5df7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_url = f\"https://api.pushshift.io/reddit/search/submission/?subreddit=funny&num_comments=>0&over_18=false&after={int(day_before_yesterday)}&before={int(yesterday)}&is_video=true&sort_type=score&sort=score:desc&size=100&aggs=subreddit\"\n",
    "\n",
    "request = requests.get(base_url)\n",
    "b = request.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b29ba50-c3fd-4169-8f27-6ee104843dbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b['data'][59]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f08483-3dd1-4000-b8bd-f14bd76c31de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for post in a['data']:    \n",
    "    pp.pprint(post)\n",
    "    break\n",
    "\n",
    "   \n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922df849-2007-41e7-869d-9a3cd2d06835",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c136a5-271f-49c5-8336-8f1f4f540a27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3810jvsc74a57bd024b555649d86f360bc29675e4ea05747664583e96f16283f97272f664e30bb4e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
