{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16f7f114-a2fd-4492-bdb1-1428a0c311bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, boto3, pandas as pd, os, sys, pprint\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# from entities.RedditAccount import RedditAccount\n",
    "# from entities.DailyUpload import DailyUpload\n",
    "\n",
    "#TODO: Change path accordingly in handler\n",
    "\n",
    "\n",
    "# Making the current directory in which this file is in discoverable to python.\n",
    "# Commenting it here because it will not work in jupyter notebook. It will work in lambda though.\n",
    "#sys.path.append(os.path.join(os.path.dirname(__file__)))\n",
    "\n",
    "# Below should be used only in jupyter notebook\n",
    "sys.path.append('../')\n",
    "\n",
    "\n",
    "REDDIT_AUTH_URL = 'https://www.reddit.com/api/v1/access_token'\n",
    "REDDIT_ACCOUNTS_TABLE_NAME = 'RedditAccountsTable-dev'\n",
    "DAILY_UPLOADS_TABLE_NAME = \"DailyUploadsTable-dev\"\n",
    "REDDIT_API_URL_TOP = \"https://oauth.reddit.com/r/placeholder_value/top\"\n",
    "REDDIT_API_URL_SORT = \"https://oauth.reddit.com/r/placeholder_value/sort\"\n",
    "\n",
    "ddb = boto3.client(\"dynamodb\", region_name=\"ap-south-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b901c5f3-2d8e-43c3-84c1-03d38753ebc7",
   "metadata": {},
   "source": [
    "# GatherUrls Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf9194a4-bff5-4596-a510-69dc6056731d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2, compact=True, width=80)\n",
    "\n",
    "\n",
    "class GatherUrls:\n",
    "    post_keys_to_keep = [\n",
    "        \"title\",\n",
    "        \"url\",\n",
    "        \"upvote_ratio\",\n",
    "        \"ups\",\n",
    "        \"author\",\n",
    "        \"name\",\n",
    "        \"total_awards_received\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self, subreddit, logger) -> None:\n",
    "        self.subreddit = subreddit\n",
    "        self.date = str(datetime.today().date())  ## Of the format yyyy-mm-dd\n",
    "        self.total_duration = 0\n",
    "        self.urls = []\n",
    "        self.latest_post = None\n",
    "        self.eligible_posts = []\n",
    "        self.logger = logger\n",
    "\n",
    "    # Renamed from date_subreddit_key()\n",
    "    def key(self) -> dict:\n",
    "        \"\"\"Returns a dictionary with date as PK, subreddit as SK.\n",
    "\n",
    "        Returns:\n",
    "            Dict: Containing serialized subreddit and date\n",
    "        \"\"\"\n",
    "\n",
    "        return {\n",
    "            \"PK\": GatherUrls.__serialize_date(self.date),\n",
    "            \"SK\": GatherUrls.__serialize_subreddit(self.subreddit),\n",
    "        }\n",
    "\n",
    "    def serialize_to_item(self):\n",
    "        \"\"\"Serializes member variable data of this object for the access pattern:\n",
    "        date-Partition Key\n",
    "        subreddit- Sort Key\n",
    "\n",
    "        Returns:\n",
    "            Dict: Ready to be used by boto3 to insert item into DynamoDB.\n",
    "        \"\"\"\n",
    "        item = self.key()\n",
    "        item[\"posts\"] = GatherUrls.__serialize_posts(self.eligible_posts)\n",
    "        self.logger.info(\"Serialized item is:\\n\")\n",
    "        self.logger.info(pp.pformat(item))\n",
    "        return item\n",
    "\n",
    "    @staticmethod\n",
    "    def __removed_post_is_worthy(post):\n",
    "        if post[\"removed_by\"] or post[\"removal_reason\"]:\n",
    "            if post[\"num_comments\"] > 5 and post[\"score\"] > 10:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    @staticmethod\n",
    "    def __is_eligible(post):\n",
    "        if post[\"is_video\"] and not post[\"over_18\"] and not post[\"stickied\"]:\n",
    "            if post[\"total_awards_received\"] > 0:\n",
    "                return True\n",
    "\n",
    "            if post[\"ups\"] > 0 and post[\"num_comments\"] > 0:\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def parse_posts(self, posts):\n",
    "        \"\"\"Parse posts and insert into a dataframe.\n",
    "        The last parsed post will updated in a member variable.\n",
    "\n",
    "        Args:\n",
    "            posts (list): List of posts from reddit API\n",
    "        \"\"\"\n",
    "        posts = posts[\"data\"][\"children\"]\n",
    "        self.logger.info(f\"For {self.subreddit} on date: {self.date}\")\n",
    "        duration = 0\n",
    "        for post in posts:\n",
    "            post = post[\"data\"]\n",
    "            self.latest_post = post\n",
    "            if GatherUrls.__is_eligible(post) and GatherUrls.__removed_post_is_worthy(\n",
    "                post\n",
    "            ):\n",
    "\n",
    "                temp = {key: post[key] for key in GatherUrls.post_keys_to_keep}\n",
    "                self.eligible_posts.append(temp)\n",
    "                duration = int(post[\"media\"][\"reddit_video\"][\"duration\"])\n",
    "                self.total_duration += duration\n",
    "                self.logger.info(\n",
    "                    f\"Post:\\nTitle: {post['title']}\\nDuration: {duration}s\\nwas added to eligible posts\\n\"\n",
    "                )\n",
    "\n",
    "        self.logger.info(\n",
    "            f\"Total duration for {self.subreddit} subreddit on {self.date} is {self.total_duration}\\n\"\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def __serialize_posts(posts):\n",
    "        serialized_posts = {\"L\": [GatherUrls.__serialize_post(post) for post in posts]}\n",
    "        return serialized_posts\n",
    "\n",
    "    @staticmethod\n",
    "    def deserialize_from_item(serialized_item):\n",
    "        deserialized_item = {}\n",
    "        serialized_item = serialized_item[\"Item\"]\n",
    "\n",
    "        for key, value in serialized_item.items():\n",
    "            for _key, _value in value.items():\n",
    "                deserialized_item[key] = helpers.ddb.deserialize_piece_of_item(\n",
    "                    _key, _value\n",
    "                )\n",
    "\n",
    "        return deserialized_item\n",
    "\n",
    "    @staticmethod\n",
    "    def __serialize_post(post):\n",
    "        serialized_post = {\"M\": {}}\n",
    "\n",
    "        for key in GatherUrls.post_keys_to_keep:\n",
    "            serialized_post[\"M\"][key] = {\n",
    "                helpers.ddb.get_datatype(post[key]): str(post[key])\n",
    "            }\n",
    "\n",
    "        return serialized_post\n",
    "\n",
    "    @staticmethod\n",
    "    def __serialize_subreddit(subreddit):\n",
    "        return {\"S\": subreddit}\n",
    "\n",
    "    @staticmethod\n",
    "    def __serialize_date(date):\n",
    "        return {\"S\": date}\n",
    "\n",
    "    @staticmethod\n",
    "    def deserialize_PK_SK_count(item):\n",
    "        deserialized_item = {}\n",
    "        for key, value in item.items():\n",
    "            for _key, _value in value.items():\n",
    "                deserialized_item[key] = _value\n",
    "        return deserialized_item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91816505-746c-4314-95ac-c8a900942e89",
   "metadata": {},
   "source": [
    "# Reddit Account Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b06066cc-4d33-40ff-b38c-aa42bb44a700",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests, logging, pprint\n",
    "from helpers.Exceptions import InvalidCredentialsProvidedException\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2, compact=True, width=80)\n",
    "\n",
    "\n",
    "class RedditAccount:\n",
    "    def __init__(self, subreddit, ddb, logger):\n",
    "        self.subreddit = subreddit\n",
    "        self.client_id = None\n",
    "        self.secret_key = None\n",
    "        self.username = None\n",
    "        self.password = None\n",
    "        self.auth = None\n",
    "        self.headers = {\"User-Agent\": f\"{subreddit}API/0.0.1\"}\n",
    "        self.data = {\"grant_type\": \"password\", \"username\": None, \"password\": None}\n",
    "        self.access_token = None\n",
    "        self.ddb = ddb\n",
    "        self.logger = logger\n",
    "\n",
    "    def key(self):\n",
    "        return {\"PK\": {\"S\": self.subreddit}}\n",
    "\n",
    "    def fetch_and_update_account_details(self, REDDIT_ACCOUNTS_TABLE_NAME):\n",
    "        params = {\"TableName\": REDDIT_ACCOUNTS_TABLE_NAME, \"Key\": self.key()}\n",
    "        item = ddb_helpers.get_item(ddb=self.ddb, logger=self.logger, **params)   \n",
    "        deserialized_item = RedditAccount.deserialize_item(item)       \n",
    "        \n",
    "        self.client_id = deserialized_item[\"personal_use_script\"]\n",
    "        self.secret_key = deserialized_item[\"secret_key\"]\n",
    "        self.username = deserialized_item[\"username\"]\n",
    "        self.password = deserialized_item[\"password\"]\n",
    "        self.data[\"username\"] = self.username\n",
    "        self.data[\"password\"] = self.password\n",
    "        self.logger.info(\"Fetched and updated the following account details:\\n\")\n",
    "        self.logger.info(pp.pformat(deserialized_item))\n",
    "\n",
    "    @staticmethod\n",
    "    def deserialize_item(item):\n",
    "        deserialized_item = {}\n",
    "        for key, value in item.items():\n",
    "            for _key, _value in value.items():\n",
    "                deserialized_item[key] = ddb_helpers.deserialize_piece_of_item(_key, _value)\n",
    "        \n",
    "\n",
    "        return deserialized_item\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_value(dictionary):\n",
    "        data_type, value = list(dictionary.keys())[0], list(dictionary.values())[0]\n",
    "\n",
    "        if data_type == \"S\":\n",
    "            return value\n",
    "\n",
    "    def authenticate_with_api(self):\n",
    "        self.auth = requests.auth.HTTPBasicAuth(self.client_id, self.secret_key)\n",
    "\n",
    "    def fetch_and_update_access_token(self, REDDIT_AUTH_URL):\n",
    "        try:\n",
    "            # Authorise and request for access token from Reddit API\n",
    "            res = requests.post(\n",
    "                REDDIT_AUTH_URL, auth=self.auth, data=self.data, headers=self.headers\n",
    "            )\n",
    "\n",
    "            res = res.json()\n",
    "            if \"error\" in res and res[\"error\"] == 401:\n",
    "                raise InvalidCredentialsProvidedException()\n",
    "\n",
    "        except (InvalidCredentialsProvidedException, Exception):\n",
    "            self.logger.error(f\"Response object contains:\\n\")\n",
    "            self.logger.error(pp.pformat(res))\n",
    "            self.logger.error(\n",
    "                \"Invalid Credentials. The following details were provided:\\n\"\n",
    "            )\n",
    "            self.logger.error(\n",
    "                f\"Requests auth object:\\nusername: {self.auth.username}\\npassword: {self.auth.password}\\n\"\n",
    "            )\n",
    "            self.logger.error(f\"Data provided in the POST request:\\n\")\n",
    "            self.logger.error(pp.pformat(self.headers))\n",
    "            self.logger.error(f\"Headers present in the POST request:\\n\")\n",
    "            self.logger.error(pp.pformat(self.headers))\n",
    "\n",
    "        self.access_token = res[\"access_token\"]\n",
    "        self.headers[\"Authorization\"] = f\"bearer {self.access_token}\"\n",
    "\n",
    "    def fetch_posts_as_json(self, url, params={}):\n",
    "        try:\n",
    "            res = requests.get(url, headers=self.headers, params=params)\n",
    "            return res.json()\n",
    "\n",
    "        except Exception as err:\n",
    "            self.logger.error(f\"Unable to fetch posts from Reddit\")\n",
    "            self.logger.error(\"Headers used:\\n\")\n",
    "            self.logger.error(pp.pformat(self.headers))\n",
    "            self.logger.error(f\"URL to fetch posts from: {url}\\n\")\n",
    "            self.logger.error(\"params passed were:\\n\")\n",
    "            self.logger.error(pp.pformat(params))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a2a386-a53f-4050-91ca-3cdf03deabb0",
   "metadata": {},
   "source": [
    "# Event handler code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a4eb313-dbf7-48f1-80c5-bc7448e37c84",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3, os, sys, logging, pprint\n",
    "from pathlib import Path\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2, compact=True, width=80)\n",
    "\n",
    "# Initialize log config.\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Making the current directory in which this file is in discoverable to python\n",
    "# sys.path.append(os.path.join(os.path.dirname(__file__)))\n",
    "\n",
    "# TODO: Commenting this only for jupyter\n",
    "# from entities.GatherUrls import GatherUrls\n",
    "# from entities.RedditAccount import RedditAccount\n",
    "from helpers import ddb as ddb_helpers\n",
    "\n",
    "# from subreddit_groups import subreddit_groups\n",
    "\n",
    "ddb = boto3.client(\"dynamodb\", region_name=\"ap-south-1\")\n",
    "sqs = boto3.client(\"sqs\")\n",
    "\n",
    "# REDDIT_AUTH_URL = os.getenv(\"REDDIT_AUTH_URL\")\n",
    "# REDDIT_ACCOUNTS_TABLE_NAME = os.getenv(\"REDDIT_ACCOUNTS_TABLE_NAME\")\n",
    "# DAILY_UPLOADS_TABLE_NAME = os.getenv(\"DAILY_UPLOADS_TABLE_NAME\")\n",
    "# PROCESS_URLS_FOR_SUBREDDIT_GROUP_QUEUE_URL = os.getenv(\n",
    "#     \"PROCESS_URLS_FOR_SUBREDDIT_GROUP_QUEUE_URL\"\n",
    "# )\n",
    "\n",
    "\n",
    "def run(event, context):\n",
    "\n",
    "    # TODO: Hardcoding subreddit value for now. In production, should extract from queue:\n",
    "    # subreddit = \"funny\"\n",
    "    subreddit = str(event[\"Records\"][0][\"body\"])\n",
    "    logger.info(f\"Subreddit : {subreddit}, is being processed\")\n",
    "\n",
    "    # Getting from env here because, if container is warm, it will fetch from the previously\n",
    "    # executed subreddit url.\n",
    "#     REDDIT_API_URL_TOP = os.getenv(\"REDDIT_API_URL_TOP\")\n",
    "    #TODO: Uncomment above and comment below one. This change is only for jupyter\n",
    "    REDDIT_API_URL_TOP = \"https://oauth.reddit.com/r/placeholder_value/top\"\n",
    "    REDDIT_API_URL_TOP = REDDIT_API_URL_TOP.replace(\"placeholder_value\", subreddit)\n",
    "\n",
    "    gather_urls = GatherUrls(subreddit=subreddit, logger=logger)\n",
    "    reddit_account = RedditAccount(subreddit=subreddit, ddb=ddb, logger=logger)\n",
    "\n",
    "    reddit_account.fetch_and_update_account_details(REDDIT_ACCOUNTS_TABLE_NAME)\n",
    "    reddit_account.authenticate_with_api()\n",
    "    reddit_account.fetch_and_update_access_token(REDDIT_AUTH_URL)\n",
    "\n",
    "    # Keep fetching and parsing posts from reddit api till gather_urls.total_duration\n",
    "    # is more than 600 seconds. Will use the 'after' param to keep going backwards.\n",
    "    after = None\n",
    "    while gather_urls.total_duration < 601:\n",
    "        logger.info(f\"Fetching {subreddit} posts after {after}\")\n",
    "        posts = reddit_account.fetch_posts_as_json(\n",
    "            REDDIT_API_URL_TOP, params={\"limit\": \"100\", \"after\": after}\n",
    "        )\n",
    "        gather_urls.parse_posts(posts)\n",
    "        after = gather_urls.latest_post[\"name\"]\n",
    "\n",
    "    # After uploading this subreddits' urls, update the count of todays_subreddits_count\n",
    "    # doing this as a transaction.\n",
    "    params = {\n",
    "        \"TransactItems\": [\n",
    "            {\n",
    "                \"Put\": {\n",
    "                    \"TableName\": DAILY_UPLOADS_TABLE_NAME,\n",
    "                    \"Item\": gather_urls.serialize_to_item(),\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"Update\": {\n",
    "                    \"TableName\": DAILY_UPLOADS_TABLE_NAME,\n",
    "                    \"Key\": {\n",
    "                        \"PK\": {\"S\": gather_urls.date},\n",
    "                        \"SK\": {\"S\": \"todays_subreddits_count\"},\n",
    "                    },\n",
    "                    \"ConditionExpression\": \"attribute_exists(PK) and attribute_exists(SK)\",\n",
    "                    \"UpdateExpression\": \"SET #count = #count + :inc\",\n",
    "                    \"ExpressionAttributeNames\": {\"#count\": \"count\"},\n",
    "                    \"ExpressionAttributeValues\": {\":inc\": {\"N\": \"1\"}},\n",
    "                }\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    res = ddb_helpers.transact_write_items(ddb, logger, **params)\n",
    "\n",
    "    logger.info(\n",
    "        f\"Successfully updated DB for {subreddit} subreddit on {gather_urls.date}\"\n",
    "    )\n",
    "   \n",
    "\n",
    "    return {\n",
    "        subreddit: f\"successfully processed {subreddit} for date: {gather_urls.date}\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5625d0d-7c73-4aeb-8bf1-ce48b8f0d477",
   "metadata": {},
   "outputs": [],
   "source": [
    "event = {\"Records\": [ {\"body\": \"funny\"} ]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6528bca5-72e4-4def-a146-a9efe54ee16e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Subreddit : funny, is being processed\n",
      "INFO:root:Received the following item:\n",
      "INFO:root:{ 'PK': {'S': 'funny'},\n",
      "  'email_address': {'S': 'mugblsxlqpoqbclaqi@bptfp.net'},\n",
      "  'password': {'S': 'Abcd@12349'},\n",
      "  'personal_use_script': {'S': 'epsNZpypuQ1IngadWDnlGg'},\n",
      "  'secret_key': {'S': 'yG4Ej57nkBMIdFzpWkzhNXChnsiluw'},\n",
      "  'username': {'S': 'mugblsxlqpoqbclaqi'}}\n",
      "INFO:root:Fetched and updated the following account details:\n",
      "\n",
      "INFO:root:{ 'PK': 'funny',\n",
      "  'email_address': 'mugblsxlqpoqbclaqi@bptfp.net',\n",
      "  'password': 'Abcd@12349',\n",
      "  'personal_use_script': 'epsNZpypuQ1IngadWDnlGg',\n",
      "  'secret_key': 'yG4Ej57nkBMIdFzpWkzhNXChnsiluw',\n",
      "  'username': 'mugblsxlqpoqbclaqi'}\n",
      "INFO:root:Fetching funny posts after None\n",
      "INFO:root:For funny on date: 2021-08-15\n",
      "INFO:root:Post:\n",
      "Title: No fucks given at the Home Depot today. 🤣 I'm envious of this man's confidence.\n",
      "Duration: 4s\n",
      "was added to eligible posts\n",
      "\n",
      "INFO:root:Post:\n",
      "Title: Apple: We're not scanning your images, we're just scanning your images.\n",
      "Duration: 38s\n",
      "was added to eligible posts\n",
      "\n",
      "INFO:root:Post:\n",
      "Title: Protect yourself !\n",
      "Duration: 62s\n",
      "was added to eligible posts\n",
      "\n",
      "INFO:root:Post:\n",
      "Title: We all know what the punchline is going to be.\n",
      "Duration: 78s\n",
      "was added to eligible posts\n",
      "\n",
      "INFO:root:Post:\n",
      "Title: The captain must go down with the ship!\n",
      "Duration: 189s\n",
      "was added to eligible posts\n",
      "\n",
      "INFO:root:Post:\n",
      "Title: The fastest way to connect to your past lives.\n",
      "Duration: 8s\n",
      "was added to eligible posts\n",
      "\n",
      "INFO:root:Post:\n",
      "Title: Can relate (sound on)\n",
      "Duration: 24s\n",
      "was added to eligible posts\n",
      "\n",
      "INFO:root:Post:\n",
      "Title: Disappointed🤣\n",
      "Duration: 9s\n",
      "was added to eligible posts\n",
      "\n",
      "INFO:root:Post:\n",
      "Title: Armed robber uses hand sanitizer before robbing a pharmacy\n",
      "Duration: 35s\n",
      "was added to eligible posts\n",
      "\n",
      "INFO:root:Post:\n",
      "Title: An attempt at indoor climbing\n",
      "Duration: 7s\n",
      "was added to eligible posts\n",
      "\n",
      "INFO:root:Post:\n",
      "Title: solving a crossword with Dom\n",
      "Duration: 103s\n",
      "was added to eligible posts\n",
      "\n",
      "INFO:root:Post:\n",
      "Title: You can't park there, sir!\n",
      "Duration: 85s\n",
      "was added to eligible posts\n",
      "\n",
      "INFO:root:Post:\n",
      "Title: Power generator on life support\n",
      "Duration: 15s\n",
      "was added to eligible posts\n",
      "\n",
      "INFO:root:Post:\n",
      "Title: This is how my cat sits on the sofa 😂🤣🐱😹\n",
      "Duration: 14s\n",
      "was added to eligible posts\n",
      "\n",
      "INFO:root:Post:\n",
      "Title: PUT IT DOWN\n",
      "Duration: 22s\n",
      "was added to eligible posts\n",
      "\n",
      "INFO:root:Post:\n",
      "Title: Local Dunkin Donuts, It was having a party of its own.\n",
      "Duration: 7s\n",
      "was added to eligible posts\n",
      "\n",
      "INFO:root:Post:\n",
      "Title: These guys are called KRPP or Kiddy Ride Police Patrol and ride around town. Checking up on kiddy rides, scootmobiles and cute pets. They have their own soundtrack.\n",
      "Duration: 23s\n",
      "was added to eligible posts\n",
      "\n",
      "INFO:root:Post:\n",
      "Title: You and i have unfinished buisness\n",
      "Duration: 12s\n",
      "was added to eligible posts\n",
      "\n",
      "INFO:root:Post:\n",
      "Title: Kick Buttowski 🤟\n",
      "Duration: 14s\n",
      "was added to eligible posts\n",
      "\n",
      "INFO:root:Post:\n",
      "Title: Was not expecting that.\n",
      "Duration: 9s\n",
      "was added to eligible posts\n",
      "\n",
      "INFO:root:Total duration for funny subreddit on 2021-08-15 is 758\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'DAILY_UPLOADS_TABLE_NAME' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_53752/2354267148.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_53752/2511468851.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(event, context)\u001b[0m\n\u001b[1;32m     67\u001b[0m             {\n\u001b[1;32m     68\u001b[0m                 \"Put\": {\n\u001b[0;32m---> 69\u001b[0;31m                     \u001b[0;34m\"TableName\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDAILY_UPLOADS_TABLE_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m                     \u001b[0;34m\"Item\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgather_urls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserialize_to_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DAILY_UPLOADS_TABLE_NAME' is not defined"
     ]
    }
   ],
   "source": [
    "run(event, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7f8dfd-e4f7-42aa-98be-77bb8832f375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96ce25a-a3a6-49a0-b842-f129309afa07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "203dc3cd-cc07-484e-93ea-6daa6dade353",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pushshift api tryout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31118117-2a1f-49a6-ba21-9163a3c27bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = PushshiftAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6780af-ffbd-4d76-868e-88cd716a727a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1335ff-b6f0-45fb-8bc6-efdecbe2c0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "today =  datetime.today()\n",
    "yesterday = datetime.today() - timedelta(days=1)\n",
    "day_before_yesterday = yesterday - timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1d29ae-d9b8-4cf4-a259-3ef9a9a0f556",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime(today.year, today.month, today.day, 0,0,0).timestamp()\n",
    "yesterday = datetime(yesterday.year, yesterday.month, yesterday.day,0,0,0).timestamp()\n",
    "day_before_yesterday = datetime(day_before_yesterday.year, day_before_yesterday.month, day_before_yesterday.day,0,0,0).timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4987782-469c-449b-a215-71e1391add11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " a = list(api.search_submissions(after=day_before_yesterday, before=today, subreddit='funny', filter=['url', 'title'], limit = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5525ffc-0bf2-4f0b-aba8-f9eb5db3fecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "int(day_before_yesterday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b94ffc6-7d9b-488d-a1eb-bd06047d62d4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_pushshift_data(data_type, **kwargs):\n",
    "    \"\"\"\n",
    "    Gets data from the pushshift api.\n",
    " \n",
    "    data_type can be 'comment' or 'submission'\n",
    "    The rest of the args are interpreted as payload.\n",
    " \n",
    "    Read more: https://github.com/pushshift/api\n",
    "    \"\"\"\n",
    " \n",
    "    base_url = f\"https://api.pushshift.io/reddit/search/submission/?subreddit=funny&num_comments=>0&after={int(day_before_yesterday)}&before={int(yesterday)}&is_video=true&sort_type=score&sort=score:asc&size=100&aggs=subreddit\"\n",
    "#     payload = {}\n",
    "#     print(payload)\n",
    "    request = requests.get(base_url)\n",
    "    return request.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7162332a-ce32-487a-91f1-729889c4332d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_type=\"submission\"     # give me comments, use \"submission\" to publish something\n",
    "query=\"funny\"          # Add your query\n",
    "duration=\"1d\"          # Select the timeframe. Epoch value or Integer + \"s,m,h,d\" (i.e. \"second\", \"minute\", \"hour\", \"day\")\n",
    "size=1000               # maximum 1000 comments\n",
    "sort_type=\"score\"       # Sort by score (Accepted: \"score\", \"num_comments\", \"created_utc\")\n",
    "sort=\"desc\"             # sort descending\n",
    "aggs=\"subreddit\"        #\"author\", \"link_id\", \"created_utc\", \"subreddit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3645ffb5-554d-47e6-a73d-f52197fb5df7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_url = f\"https://api.pushshift.io/reddit/search/submission/?subreddit=funny&num_comments=>0&over_18=false&after={int(day_before_yesterday)}&before={int(yesterday)}&is_video=true&sort_type=score&sort=score:desc&size=100&aggs=subreddit\"\n",
    "\n",
    "request = requests.get(base_url)\n",
    "b = request.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b29ba50-c3fd-4169-8f27-6ee104843dbf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "b['data'][59]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f08483-3dd1-4000-b8bd-f14bd76c31de",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for post in a['data']:    \n",
    "    pp.pprint(post)\n",
    "    break\n",
    "\n",
    "   \n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "922df849-2007-41e7-869d-9a3cd2d06835",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "serialized_item = { 'PK': {'S': '2021-08-16'},\n",
    "'SK': {'S': 'funny'},\n",
    "'posts': { 'L': [ { 'M': { 'author': {'S': 'Lurkingredditatwork'},\n",
    "'name': {'S': 't3_p4m9s0'},\n",
    "'title': { 'S': \"Apple: We're not scanning your \"\n",
    "\"images, we're just scanning your \"\n",
    "'images.'},\n",
    "'total_awards_received': {'N': '14'},\n",
    "'ups': {'N': '9632'},\n",
    "'upvote_ratio': {'N': '0.94'},\n",
    "'url': {'S': 'https://v.redd.it/z3vy8b710gh71'}}},\n",
    "{ 'M': { 'author': {'S': 'SplungerPlunger'},\n",
    "'name': {'S': 't3_p549q6'},\n",
    "'title': {'S': 'my best goodwill find'},\n",
    "'total_awards_received': {'N': '37'},\n",
    "'ups': {'N': '27688'},\n",
    "'upvote_ratio': {'N': '0.95'},\n",
    "'url': {'S': 'https://v.redd.it/jjv7d2hdwlh71'}}},\n",
    "{ 'M': { 'author': {'S': 'nocopyrightstuffs'},\n",
    "'name': {'S': 't3_p4qmkd'},\n",
    "'title': {'S': 'Disappointed🤣'},\n",
    "'total_awards_received': {'N': '0'},\n",
    "'ups': {'N': '1163'},\n",
    "'upvote_ratio': {'N': '0.88'},\n",
    "'url': {'S': 'https://v.redd.it/fte9rb3puhh71'}}},\n",
    "{ 'M': { 'author': {'S': 'osrppp'},\n",
    "'name': {'S': 't3_p4l82n'},\n",
    "'title': {'S': 'Can relate (sound on)'},\n",
    "'total_awards_received': {'N': '2'},\n",
    "'ups': {'N': '340'},\n",
    "'upvote_ratio': {'N': '0.9'},\n",
    "'url': {'S': 'https://v.redd.it/5qoe5uhgnfh71'}}},\n",
    "{ 'M': { 'author': {'S': 'Mano_____'},\n",
    "'name': {'S': 't3_p51s3k'},\n",
    "'title': {'S': 'Don’t lose your focus.'},\n",
    "'total_awards_received': {'N': '2'},\n",
    "'ups': {'N': '530'},\n",
    "'upvote_ratio': {'N': '0.95'},\n",
    "'url': {'S': 'https://v.redd.it/0vqji3ew6lh71'}}},\n",
    "{ 'M': { 'author': {'S': 'Havocfyw'},\n",
    "'name': {'S': 't3_p4z3lx'},\n",
    "'title': { 'S': 'The Greatest Back Yard Wrestling '\n",
    "'Entrance of All Time'},\n",
    "'total_awards_received': {'N': '1'},\n",
    "'ups': {'N': '419'},\n",
    "'upvote_ratio': {'N': '0.92'},\n",
    "'url': {'S': 'https://v.redd.it/1gmm2kphgkh71'}}},\n",
    "{ 'M': { 'author': {'S': 'Takkenwijf87'},\n",
    "'name': {'S': 't3_p4uxpc'},\n",
    "'title': { 'S': 'These guys are called KRPP or '\n",
    "'Kiddy Ride Police Patrol and '\n",
    "'ride around town. Checking up on '\n",
    "'kiddy rides, scootmobiles and '\n",
    "'cute pets. They have their own '\n",
    "'soundtrack.'},\n",
    "'total_awards_received': {'N': '0'},\n",
    "'ups': {'N': '227'},\n",
    "'upvote_ratio': {'N': '0.92'},\n",
    "'url': {'S': 'https://v.redd.it/hmhtndidcjh71'}}},\n",
    "{ 'M': { 'author': {'S': 'jodihas2kids'},\n",
    "'name': {'S': 't3_p526ne'},\n",
    "'title': {'S': 'Puppy vs door stop'},\n",
    "'total_awards_received': {'N': '2'},\n",
    "'ups': {'N': '146'},\n",
    "'upvote_ratio': {'N': '0.91'},\n",
    "'url': {'S': 'https://v.redd.it/c070ye4ualh71'}}},\n",
    "{ 'M': { 'author': {'S': 'Yash817813'},\n",
    "'name': {'S': 't3_p4y3ts'},\n",
    "'title': { 'S': 'Penguin staring at the sunset 🌇 '\n",
    "'🐧'},\n",
    "'total_awards_received': {'N': '0'},\n",
    "'ups': {'N': '122'},\n",
    "'upvote_ratio': {'N': '0.88'},\n",
    "'url': {'S': 'https://v.redd.it/pjlafy467kh71'}}},\n",
    "{ 'M': { 'author': {'S': 'sparky_sly14'},\n",
    "'name': {'S': 't3_p4t8bu'},\n",
    "'title': { 'S': 'Local Dunkin Donuts, It was '\n",
    "'having a party of its own.'},\n",
    "'total_awards_received': {'N': '0'},\n",
    "'ups': {'N': '87'},\n",
    "'upvote_ratio': {'N': '0.85'},\n",
    "'url': {'S': 'https://v.redd.it/55oifuh1vih71'}}},\n",
    "{ 'M': { 'author': {'S': 'thistlegypsy'},\n",
    "'name': {'S': 't3_p54vum'},\n",
    "'title': { 'S': 'Difference between boys and '\n",
    "'girls'},\n",
    "'total_awards_received': {'N': '1'},\n",
    "'ups': {'N': '91'},\n",
    "'upvote_ratio': {'N': '0.78'},\n",
    "'url': {'S': 'https://v.redd.it/lx1auqcy1mh71'}}},\n",
    "{ 'M': { 'author': {'S': 'micheleberaudo'},\n",
    "'name': {'S': 't3_p4zxht'},\n",
    "'title': {'S': 'Just a normal camping day'},\n",
    "'total_awards_received': {'N': '0'},\n",
    "'ups': {'N': '60'},\n",
    "'upvote_ratio': {'N': '0.79'},\n",
    "'url': {'S': 'https://v.redd.it/bbbyix1rokh71'}}},\n",
    "{ 'M': { 'author': {'S': 'JehovasHitMan'},\n",
    "'name': {'S': 't3_p4v376'},\n",
    "'title': { 'S': 'Now a lawyers gonna get '\n",
    "'involved.'},\n",
    "'total_awards_received': {'N': '0'},\n",
    "'ups': {'N': '48'},\n",
    "'upvote_ratio': {'N': '0.69'},\n",
    "'url': {'S': 'https://v.redd.it/5227hzy1ejh71'}}},\n",
    "{ 'M': { 'author': {'S': 'BeTTish_09'},\n",
    "'name': {'S': 't3_p4ymet'},\n",
    "'title': { 'S': 'Wait for the MOM ..and her '\n",
    "'laugh!🤣🤣'},\n",
    "'total_awards_received': {'N': '0'},\n",
    "'ups': {'N': '45'},\n",
    "'upvote_ratio': {'N': '0.7'},\n",
    "'url': {'S': 'https://v.redd.it/pa8eepnzbkh71'}}},\n",
    "{ 'M': { 'author': {'S': 'manic-god'},\n",
    "'name': {'S': 't3_p4q5qs'},\n",
    "'title': {'S': 'Was not expecting that.'},\n",
    "'total_awards_received': {'N': '0'},\n",
    "'ups': {'N': '45'},\n",
    "'upvote_ratio': {'N': '0.78'},\n",
    "'url': {'S': 'https://v.redd.it/mocbj31tmhh71'}}},\n",
    "{ 'M': { 'author': {'S': 'TimHamburg'},\n",
    "'name': {'S': 't3_p4ui6t'},\n",
    "'title': { 'S': 'Round Sunglasses are for '\n",
    "'Villains: Part 2'},\n",
    "'total_awards_received': {'N': '0'},\n",
    "'ups': {'N': '37'},\n",
    "'upvote_ratio': {'N': '0.76'},\n",
    "'url': {'S': 'https://v.redd.it/ubfn2mw48jh71'}}},\n",
    "{ 'M': { 'author': {'S': 'ErockLobster'},\n",
    "'name': {'S': 't3_p5270o'},\n",
    "'title': { 'S': 'Mission ImPAWSable (original '\n",
    "'credit to u/pazluz)'},\n",
    "'total_awards_received': {'N': '0'},\n",
    "'ups': {'N': '29'},\n",
    "'upvote_ratio': {'N': '0.75'},\n",
    "'url': {'S': 'https://v.redd.it/yqpwfe0nalh71'}}},\n",
    "{ 'M': { 'author': {'S': 'Havocfyw'},\n",
    "'name': {'S': 't3_p4wt0t'},\n",
    "'title': {'S': \"Quick, she's coming! Act casual.\"},\n",
    "'total_awards_received': {'N': '0'},\n",
    "'ups': {'N': '33'},\n",
    "'upvote_ratio': {'N': '0.73'},\n",
    "'url': {'S': 'https://v.redd.it/zamfhd9iujh71'}}},\n",
    "{ 'M': { 'author': {'S': 'teyko17'},\n",
    "'name': {'S': 't3_p4u6pq'},\n",
    "'title': {'S': 'A bugs life'},\n",
    "'total_awards_received': {'N': '0'},\n",
    "'ups': {'N': '27'},\n",
    "'upvote_ratio': {'N': '0.75'},\n",
    "'url': {'S': 'https://v.redd.it/ykv0v8hw4jh71'}}},\n",
    "{ 'M': { 'author': {'S': 'BostonFan69'},\n",
    "'name': {'S': 't3_p4wmra'},\n",
    "'title': {'S': 'Supr Smsh Brs Spngbb'},\n",
    "'total_awards_received': {'N': '0'},\n",
    "'ups': {'N': '24'},\n",
    "'upvote_ratio': {'N': '0.73'},\n",
    "'url': {'S': 'https://v.redd.it/y0h75ne3tjh71'}}},\n",
    "{ 'M': { 'author': {'S': 'CarLeroy316'},\n",
    "'name': {'S': 't3_p56e0k'},\n",
    "'title': {'S': 'Comedy without speaking.'},\n",
    "'total_awards_received': {'N': '0'},\n",
    "'ups': {'N': '70'},\n",
    "'upvote_ratio': {'N': '0.87'},\n",
    "'url': {'S': 'https://v.redd.it/iz30p67wjmh71'}}},\n",
    "{ 'M': { 'author': {'S': 'fryether'},\n",
    "'name': {'S': 't3_p563ud'},\n",
    "'title': { 'S': 'Apparently my son understands a '\n",
    "'practical joke at 11 months'},\n",
    "'total_awards_received': {'N': '1'},\n",
    "'ups': {'N': '38'},\n",
    "'upvote_ratio': {'N': '0.72'},\n",
    "'url': {'S': 'https://v.redd.it/0ulzinopgmh71'}}},\n",
    "{ 'M': { 'author': {'S': 'ludorthegreat'},\n",
    "'name': {'S': 't3_p4qgze'},\n",
    "'title': { 'S': '“Dreams“ bavarian style '\n",
    "'(inspired by The Inspired '\n",
    "'Unemployed)'},\n",
    "'total_awards_received': {'N': '0'},\n",
    "'ups': {'N': '19'},\n",
    "'upvote_ratio': {'N': '0.63'},\n",
    "'url': {'S': 'https://v.redd.it/rslvgbdyrhh71'}}},\n",
    "{ 'M': { 'author': {'S': 'somethingcliched'},\n",
    "'name': {'S': 't3_p4sxbb'},\n",
    "'title': { 'S': 'Indian Daily Soap. (Loose '\n",
    "'translation in comments)'},\n",
    "'total_awards_received': {'N': '0'},\n",
    "'ups': {'N': '14'},\n",
    "'upvote_ratio': {'N': '0.66'},\n",
    "'url': {'S': 'https://v.redd.it/pp1c5q1grih71'}}},\n",
    "{ 'M': { 'author': {'S': 'gokuorkakarot'},\n",
    "'name': {'S': 't3_p4xuj4'},\n",
    "'title': {'S': 'Its the pig from loonie toons'},\n",
    "'total_awards_received': {'N': '0'},\n",
    "'ups': {'N': '17'},\n",
    "'upvote_ratio': {'N': '0.63'},\n",
    "'url': {'S': 'https://v.redd.it/atb1ud9o4kh71'}}},\n",
    "{ 'M': { 'author': {'S': 'caswunn'},\n",
    "'name': {'S': 't3_p4vrir'},\n",
    "'title': { 'S': 'Did you know that giraffes are '\n",
    "'ticklish?'},\n",
    "'total_awards_received': {'N': '0'},\n",
    "'ups': {'N': '12'},\n",
    "'upvote_ratio': {'N': '0.71'},\n",
    "'url': {'S': 'https://v.redd.it/nish3svnkjh71'}}},\n",
    "{ 'M': { 'author': {'S': 'Yogurteat'},\n",
    "'name': {'S': 't3_p4o9xa'},\n",
    "'title': {'S': 'Joseph!? Are you okay?!?'},\n",
    "'total_awards_received': {'N': '0'},\n",
    "'ups': {'N': '5'},\n",
    "'upvote_ratio': {'N': '0.57'},\n",
    "'url': {'S': 'https://v.redd.it/mr59xlhfsgh71'}}},\n",
    "{ 'M': { 'author': {'S': 'sirhyperlife'},\n",
    "'name': {'S': 't3_p4lu4y'},\n",
    "'title': { 'S': 'What better music video then '\n",
    "'this'},\n",
    "'total_awards_received': {'N': '1'},\n",
    "'ups': {'N': '11'},\n",
    "'upvote_ratio': {'N': '0.57'},\n",
    "'url': {'S': 'https://v.redd.it/qkns2vq5vfh71'}}},\n",
    "{ 'M': { 'author': {'S': 'I_DoNt_CaRE_Sorry'},\n",
    "'name': {'S': 't3_p4m4kt'},\n",
    "'title': {'S': 'Just wanted ice cream'},\n",
    "'total_awards_received': {'N': '0'},\n",
    "'ups': {'N': '5'},\n",
    "'upvote_ratio': {'N': '0.57'},\n",
    "'url': {'S': 'https://v.redd.it/l1ohykezyfh71'}}},\n",
    "{ 'M': { 'author': {'S': 'Havocfyw'},\n",
    "'name': {'S': 't3_p4ucc8'},\n",
    "'title': { 'S': 'Ancient contraceptive '\n",
    "'technique... It takes a village'},\n",
    "'total_awards_received': {'N': '0'},\n",
    "'ups': {'N': '3'},\n",
    "'upvote_ratio': {'N': '0.53'},\n",
    "'url': {'S': 'https://v.redd.it/e5aojtkx5jh71'}}},\n",
    "{ 'M': { 'author': {'S': 'GreetingsFromAP'},\n",
    "'name': {'S': 't3_p4xe2b'},\n",
    "'title': { 'S': 'I know you are, but what am I? '\n",
    "'Reminiscence trailer looks ok, '\n",
    "'but the casting is spot on.'},\n",
    "'total_awards_received': {'N': '0'},\n",
    "'ups': {'N': '4'},\n",
    "'upvote_ratio': {'N': '0.63'},\n",
    "'url': {'S': 'https://v.redd.it/egk9tqf3zjh71'}}},\n",
    "{ 'M': { 'author': {'S': 'PavlovsCat333'},\n",
    "'name': {'S': 't3_p56gqh'},\n",
    "'title': { 'S': 'Randy raccoons cavorting in my '\n",
    "'yard'},\n",
    "'total_awards_received': {'N': '1'},\n",
    "'ups': {'N': '18'},\n",
    "'upvote_ratio': {'N': '0.88'},\n",
    "'url': { 'S': 'https://v.redd.it/6kvgjphkkmh71'}}}]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40c136a5-271f-49c5-8336-8f1f4f540a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import ddb as ddb_helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b67cc34f-c3a6-4fc3-802a-c91d622d9048",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        \"TransactItems\": [\n",
    "            {\n",
    "                \"Put\": {\n",
    "                    \"TableName\": DAILY_UPLOADS_TABLE_NAME,\n",
    "                    \"Item\": serialized_item,\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"Update\": {\n",
    "                    \"TableName\": DAILY_UPLOADS_TABLE_NAME,\n",
    "                    \"Key\": {\n",
    "                        \"PK\": {\"S\": '2021-08-16'},\n",
    "                        \"SK\": {\"S\": \"todays_subreddits_count\"},\n",
    "                    },\n",
    "                    \"ConditionExpression\": \"attribute_exists(PK) and attribute_exists(SK)\",\n",
    "                    \"UpdateExpression\": \"SET #count = #count + :inc\",\n",
    "                    \"ExpressionAttributeNames\": {\"#count\": \"count\"},\n",
    "                    \"ExpressionAttributeValues\": {\":inc\": {\"N\": \"1\"}},\n",
    "                }\n",
    "            },\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "236ce312-b84b-41cb-ac32-f2c8ae810a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "                \n",
    "res = ddb_helpers.transact_write_items(ddb, logger, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5dd89717-d972-4c6c-a88a-d2f41bb4728c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'FB315MCR90UCACDN9FMRUQ120FVV4KQNSO5AEMVJF66Q9ASUAAJG',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'server': 'Server',\n",
       "   'date': 'Mon, 16 Aug 2021 03:58:37 GMT',\n",
       "   'content-type': 'application/x-amz-json-1.0',\n",
       "   'content-length': '2',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': 'FB315MCR90UCACDN9FMRUQ120FVV4KQNSO5AEMVJF66Q9ASUAAJG',\n",
       "   'x-amz-crc32': '2745614147'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b80854-ba90-426f-a0b3-ef58ad51c711",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3810jvsc74a57bd024b555649d86f360bc29675e4ea05747664583e96f16283f97272f664e30bb4e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
